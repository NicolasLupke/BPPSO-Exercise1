{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Analysis 3: Process Variant Clustering - Outcome-Based Pattern Discovery\n",
        "\n",
        "## 1. Hypotheses\n",
        "\n",
        "### Research Questions:\n",
        "- Do processes leading to different outcomes (Denied, Cancelled, Pending) have different activity patterns?\n",
        "- Can we identify distinct clusters of process variants that are associated with specific outcomes?\n",
        "- Are there perceivable differences in processes that lead to certain outcomes?\n",
        "- Which activity patterns characterize each outcome type?\n",
        "- Can we predict case outcomes based on process variant clusters?\n",
        "\n",
        "### Why this matters for prediction/simulation:\n",
        "- Understanding if different outcomes follow distinct process patterns enables more accurate outcome prediction\n",
        "- Clusters can represent different process scenarios in simulation models, each with outcome-specific probabilities\n",
        "- Identifying outcome-specific patterns helps build targeted prediction models for each outcome type\n",
        "- Early detection of outcome-specific patterns enables proactive intervention\n",
        "- Process redesign can focus on shifting cases from negative outcome clusters to positive ones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pm4py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.stats import chi2_contingency\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(\"../../Results/Advanced_Analysis/variant_clustering\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1,202,267 events\n",
            "Number of cases: 31,509\n",
            "Number of unique activities: 26\n",
            "Number of unique resources: 149\n"
          ]
        }
      ],
      "source": [
        "# Load event log\n",
        "log_path = \"../../Dataset/BPI Challenge 2017.xes\"\n",
        "log = pm4py.read_xes(log_path)\n",
        "df = pm4py.convert_to_dataframe(log)\n",
        "\n",
        "# Ensure timestamp is datetime\n",
        "df['time:timestamp'] = pd.to_datetime(df['time:timestamp'], utc=True)\n",
        "\n",
        "# Sort by case and timestamp\n",
        "df = df.sort_values(['case:concept:name', 'time:timestamp'])\n",
        "\n",
        "print(f\"Loaded {len(df):,} events\")\n",
        "print(f\"Number of cases: {df['case:concept:name'].nunique():,}\")\n",
        "print(f\"Number of unique activities: {df['concept:name'].nunique()}\")\n",
        "print(f\"Number of unique resources: {df['org:resource'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Approach and Results\n",
        "\n",
        "### Methodology:\n",
        "- Extract process variants (activity sequences) from the event log\n",
        "- Classify case outcomes based on whether endpoint activities appear anywhere in the trace\n",
        "- Create feature vectors for variants:\n",
        "  - Activity presence/absence (binary features)\n",
        "  - Normalized activity frequencies\n",
        "- Apply dimensionality reduction (PCA/t-SNE) for visualization and noise reduction\n",
        "- Perform clustering (K-means) with optimal cluster selection using silhouette score\n",
        "- Analyze cluster-outcome relationships:\n",
        "  - Visualize clusters colored by outcome to see if outcomes separate naturally\n",
        "  - Create cross-tabulations and heatmaps showing cluster-outcome associations\n",
        "  - Calculate cluster purity metrics for each outcome\n",
        "  - Apply statistical tests (chi-square) to quantify cluster-outcome associations\n",
        "- Compare activity patterns across outcome groups and clusters\n",
        "- Analyze performance metrics (duration, resource usage) by cluster and outcome\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 31,509 cases\n",
            "Number of unique variants: 15,930\n",
            "\n",
            "Outcome distribution:\n",
            "outcome\n",
            "Pending      17228\n",
            "Cancelled    10431\n",
            "Denied        3752\n",
            "Other           98\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Extract variants and case-level information\n",
        "case_data = df.groupby('case:concept:name').agg({\n",
        "    'time:timestamp': ['min', 'max'],\n",
        "    'concept:name': lambda x: list(x),  # Activity sequence\n",
        "    'org:resource': lambda x: list(x),  # Resource sequence\n",
        "    'case:LoanGoal': 'first',\n",
        "    'case:ApplicationType': 'first',\n",
        "    'case:RequestedAmount': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "case_data.columns = ['case_id', 'start_time', 'end_time', 'activity_sequence', \n",
        "                     'resource_sequence', 'loan_goal', 'app_type', 'requested_amount']\n",
        "\n",
        "# Calculate case duration\n",
        "case_data['duration_days'] = (case_data['end_time'] - case_data['start_time']).dt.total_seconds() / (24 * 3600)\n",
        "\n",
        "# Classify outcomes based on whether endpoint activities appear anywhere in the trace\n",
        "# Priority: Denied > Cancelled > Pending (if multiple endpoints appear)\n",
        "def classify_outcome(activities):\n",
        "    # Check if activities is None or empty\n",
        "    if activities is None:\n",
        "        return 'Unknown'\n",
        "    try:\n",
        "        if len(activities) == 0:\n",
        "            return 'Unknown'\n",
        "    except (TypeError, AttributeError):\n",
        "        return 'Unknown'\n",
        "    \n",
        "    # Check if any endpoint activity appears in the trace\n",
        "    activities_set = set(activities)\n",
        "    \n",
        "    if 'A_Denied' in activities_set:\n",
        "        return 'Denied'\n",
        "    if 'A_Cancelled' in activities_set:\n",
        "        return 'Cancelled'\n",
        "    if 'A_Pending' in activities_set:\n",
        "        return 'Pending'\n",
        "    return 'Other'\n",
        "\n",
        "case_data['outcome'] = case_data['activity_sequence'].apply(classify_outcome)\n",
        "\n",
        "# Create variant string for grouping\n",
        "case_data['variant_string'] = case_data['activity_sequence'].apply(lambda x: ' -> '.join(str(a) for a in x))\n",
        "\n",
        "print(f\"Prepared {len(case_data):,} cases\")\n",
        "print(f\"Number of unique variants: {case_data['variant_string'].nunique():,}\")\n",
        "print(f\"\\nOutcome distribution:\")\n",
        "print(case_data['outcome'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique activities: 26\n",
            "Feature matrix shape: (31509, 52)\n",
            "Features standardized\n"
          ]
        }
      ],
      "source": [
        "# Get all unique activities\n",
        "all_activities = set()\n",
        "for seq in case_data['activity_sequence']:\n",
        "    all_activities.update(seq)\n",
        "all_activities = sorted(list(all_activities))\n",
        "\n",
        "print(f\"Total unique activities: {len(all_activities)}\")\n",
        "\n",
        "# Create feature vectors: activity presence/absence and frequencies\n",
        "variant_features = []\n",
        "\n",
        "for idx, row in case_data.iterrows():\n",
        "    seq = row['activity_sequence']\n",
        "    \n",
        "    # Binary features: activity presence\n",
        "    activity_presence = [1 if activity in seq else 0 for activity in all_activities]\n",
        "    \n",
        "    # Frequency features: how many times each activity appears\n",
        "    activity_freq = [seq.count(activity) for activity in all_activities]\n",
        "    \n",
        "    # Normalized frequency\n",
        "    total_activities = len(seq)\n",
        "    activity_freq_norm = [freq / total_activities if total_activities > 0 else 0 for freq in activity_freq]\n",
        "    \n",
        "    # Combine features\n",
        "    features = activity_presence + activity_freq_norm\n",
        "    variant_features.append(features)\n",
        "\n",
        "# Convert to numpy array\n",
        "X = np.array(variant_features)\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Features standardized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PCA explained variance ratio (first 10 components): [0.37825921 0.11144788 0.08659109 0.06899701 0.0598742  0.04755799\n",
            " 0.04385306 0.04123079 0.03865201 0.03684944]\n",
            "PCA cumulative explained variance (first 10): [0.37825921 0.48970708 0.57629818 0.64529518 0.70516938 0.75272737\n",
            " 0.79658044 0.83781123 0.87646323 0.91331267]\n",
            "PCA total explained variance (50 components): 1.0000\n",
            "\n",
            "Computing t-SNE (this may take a while)...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TSNE.__init__() got an unexpected keyword argument 'n_iter'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Apply t-SNE for 2D visualization\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComputing t-SNE (this may take a while)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m tsne = \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m X_tsne = tsne.fit_transform(X_pca[:, :\u001b[32m30\u001b[39m])  \u001b[38;5;66;03m# Use first 30 PCA components for t-SNE\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mt-SNE completed\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: TSNE.__init__() got an unexpected keyword argument 'n_iter'"
          ]
        }
      ],
      "source": [
        "# Apply PCA for dimensionality reduction (for visualization and to reduce noise)\n",
        "pca = PCA(n_components=50)  # Keep 50 components to capture most variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"PCA explained variance ratio (first 10 components): {pca.explained_variance_ratio_[:10]}\")\n",
        "print(f\"PCA cumulative explained variance (first 10): {np.cumsum(pca.explained_variance_ratio_[:10])}\")\n",
        "print(f\"PCA total explained variance (50 components): {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# Apply t-SNE for 2D visualization\n",
        "print(\"\\nComputing t-SNE (this may take a while)...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_pca[:, :30])  # Use first 30 PCA components for t-SNE\n",
        "print(\"t-SNE completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize t-SNE colored by outcome to see if outcomes separate naturally\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "\n",
        "outcome_colors = {'Denied': 'red', 'Cancelled': 'orange', 'Pending': 'green', 'Other': 'gray', 'Unknown': 'black'}\n",
        "for outcome in case_data['outcome'].unique():\n",
        "    mask = case_data['outcome'] == outcome\n",
        "    ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
        "              c=outcome_colors.get(outcome, 'gray'), label=outcome, \n",
        "              alpha=0.6, s=10, edgecolors='black', linewidth=0.1)\n",
        "\n",
        "ax.set_xlabel('t-SNE Component 1', fontsize=12)\n",
        "ax.set_ylabel('t-SNE Component 2', fontsize=12)\n",
        "ax.set_title('Process Variants in t-SNE Space (Colored by Outcome)', \n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Outcome', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'tsne_colored_by_outcome.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: tsne_colored_by_outcome.png\")\n",
        "\n",
        "# Determine optimal number of clusters using silhouette score\n",
        "# Test different numbers of clusters\n",
        "n_clusters_range = range(3, 15)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_clusters in n_clusters_range:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_pca[:, :30])  # Use first 30 PCA components\n",
        "    silhouette_avg = silhouette_score(X_pca[:, :30], cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "    print(f\"n_clusters={n_clusters}: silhouette_score={silhouette_avg:.4f}\")\n",
        "\n",
        "# Find optimal number of clusters\n",
        "optimal_n = n_clusters_range[np.argmax(silhouette_scores)]\n",
        "print(f\"\\nOptimal number of clusters (based on silhouette score): {optimal_n}\")\n",
        "\n",
        "# Plot silhouette scores\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(n_clusters_range, silhouette_scores, marker='o', linewidth=2, markersize=8)\n",
        "ax.axvline(x=optimal_n, color='r', linestyle='--', label=f'Optimal: {optimal_n}')\n",
        "ax.set_xlabel('Number of Clusters', fontsize=12)\n",
        "ax.set_ylabel('Silhouette Score', fontsize=12)\n",
        "ax.set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'silhouette_scores.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: silhouette_scores.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform K-means clustering with optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=optimal_n, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_pca[:, :30])\n",
        "\n",
        "case_data['cluster'] = cluster_labels\n",
        "\n",
        "print(f\"Clustering completed: {optimal_n} clusters\")\n",
        "print(f\"Cluster sizes:\")\n",
        "print(case_data['cluster'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize clusters vs outcomes\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Plot 1: t-SNE colored by outcome\n",
        "outcome_colors = {'Denied': 'red', 'Cancelled': 'orange', 'Pending': 'green', 'Other': 'gray', 'Unknown': 'black'}\n",
        "for outcome in case_data['outcome'].unique():\n",
        "    mask = case_data['outcome'] == outcome\n",
        "    axes[0].scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
        "                   c=outcome_colors.get(outcome, 'gray'), label=outcome, \n",
        "                   alpha=0.6, s=10, edgecolors='black', linewidth=0.1)\n",
        "axes[0].set_xlabel('t-SNE Component 1', fontsize=12)\n",
        "axes[0].set_ylabel('t-SNE Component 2', fontsize=12)\n",
        "axes[0].set_title('t-SNE: Colored by Outcome', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(title='Outcome', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: t-SNE colored by cluster\n",
        "scatter = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, \n",
        "                         cmap='tab10', s=10, alpha=0.6, edgecolors='black', linewidth=0.1)\n",
        "axes[1].set_xlabel('t-SNE Component 1', fontsize=12)\n",
        "axes[1].set_ylabel('t-SNE Component 2', fontsize=12)\n",
        "axes[1].set_title(f't-SNE: Colored by Cluster ({optimal_n} clusters)', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'tsne_clusters_vs_outcomes.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: tsne_clusters_vs_outcomes.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze cluster-outcome relationship\n",
        "# Create cross-tabulation: Cluster vs Outcome\n",
        "cluster_outcome_crosstab = pd.crosstab(case_data['cluster'], case_data['outcome'], normalize='index')\n",
        "cluster_outcome_counts = pd.crosstab(case_data['cluster'], case_data['outcome'])\n",
        "\n",
        "print(\"Cluster-Outcome Cross-tabulation (Proportions):\")\n",
        "print(cluster_outcome_crosstab.round(3))\n",
        "print(\"\\nCluster-Outcome Cross-tabulation (Counts):\")\n",
        "print(cluster_outcome_counts)\n",
        "\n",
        "# Calculate cluster purity for each outcome\n",
        "# Purity: percentage of cases in a cluster that belong to a specific outcome\n",
        "cluster_purity = {}\n",
        "for outcome in ['Denied', 'Cancelled', 'Pending', 'Other']:\n",
        "    cluster_purity[outcome] = cluster_outcome_crosstab[outcome].max()\n",
        "\n",
        "print(\"\\nCluster Purity (Maximum proportion of each outcome in any cluster):\")\n",
        "for outcome, purity in cluster_purity.items():\n",
        "    cluster_id = cluster_outcome_crosstab[outcome].idxmax()\n",
        "    print(f\"  {outcome}: {purity:.4f} (Cluster {cluster_id})\")\n",
        "\n",
        "# Save cross-tabulation\n",
        "cluster_outcome_crosstab.to_csv(output_dir / 'cluster_outcome_crosstab_proportions.csv')\n",
        "cluster_outcome_counts.to_csv(output_dir / 'cluster_outcome_crosstab_counts.csv')\n",
        "print(\"\\nSaved: cluster_outcome_crosstab_proportions.csv\")\n",
        "print(\"Saved: cluster_outcome_crosstab_counts.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cluster-outcome relationship\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# Plot 1: Heatmap of cluster vs outcome (proportions)\n",
        "sns.heatmap(cluster_outcome_crosstab, annot=True, fmt='.3f', cmap='YlOrRd', \n",
        "           cbar_kws={'label': 'Proportion'}, ax=axes[0, 0])\n",
        "axes[0, 0].set_xlabel('Outcome', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Cluster', fontsize=12)\n",
        "axes[0, 0].set_title('Cluster-Outcome Association (Proportions)', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Plot 2: Stacked bar chart - Outcome distribution within each cluster\n",
        "cluster_outcome_crosstab.plot(kind='bar', stacked=True, ax=axes[0, 1], \n",
        "                              colormap='Set2', width=0.8)\n",
        "axes[0, 1].set_xlabel('Cluster', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Proportion', fontsize=12)\n",
        "axes[0, 1].set_title('Outcome Distribution within Each Cluster', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend(title='Outcome', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Plot 3: Grouped bar chart - Cluster distribution for each outcome\n",
        "outcome_cluster_crosstab = pd.crosstab(case_data['outcome'], case_data['cluster'], normalize='index')\n",
        "outcome_cluster_crosstab.plot(kind='bar', ax=axes[1, 0], width=0.8, colormap='tab10')\n",
        "axes[1, 0].set_xlabel('Outcome', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Proportion', fontsize=12)\n",
        "axes[1, 0].set_title('Cluster Distribution for Each Outcome', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 4: Cluster sizes and dominant outcome\n",
        "cluster_sizes = case_data['cluster'].value_counts().sort_index()\n",
        "dominant_outcomes = cluster_outcome_crosstab.idxmax(axis=1)\n",
        "cluster_summary = pd.DataFrame({\n",
        "    'size': cluster_sizes,\n",
        "    'dominant_outcome': dominant_outcomes\n",
        "})\n",
        "\n",
        "x_pos = np.arange(len(cluster_summary))\n",
        "colors_map = {'Denied': 'red', 'Cancelled': 'orange', 'Pending': 'green', 'Other': 'gray'}\n",
        "colors = [colors_map.get(outcome, 'black') for outcome in cluster_summary['dominant_outcome']]\n",
        "axes[1, 1].bar(x_pos, cluster_summary['size'], color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Cluster', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Number of Cases', fontsize=12)\n",
        "axes[1, 1].set_title('Cluster Sizes (Colored by Dominant Outcome)', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(cluster_summary.index)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add text labels for dominant outcome\n",
        "for i, (idx, row) in enumerate(cluster_summary.iterrows()):\n",
        "    axes[1, 1].text(i, row['size'] + 50, row['dominant_outcome'], \n",
        "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'cluster_outcome_relationship_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: cluster_outcome_relationship_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical analysis of cluster-outcome association\n",
        "# Chi-square test to determine if cluster membership is significantly associated with outcome\n",
        "chi2, p_value, dof, expected = chi2_contingency(cluster_outcome_counts)\n",
        "\n",
        "print(\"=== Statistical Test: Cluster-Outcome Association ===\")\n",
        "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
        "print(f\"p-value: {p_value:.2e}\")\n",
        "print(f\"Degrees of freedom: {dof}\")\n",
        "print(f\"\\nInterpretation: {'Significant association' if p_value < 0.05 else 'No significant association'}\")\n",
        "print(f\"  (p < 0.05 indicates cluster membership is significantly associated with outcome)\")\n",
        "\n",
        "# Calculate Cramér's V (measure of association strength)\n",
        "n = cluster_outcome_counts.sum().sum()\n",
        "cramers_v = np.sqrt(chi2 / (n * (min(cluster_outcome_counts.shape) - 1)))\n",
        "print(f\"\\nCramér's V: {cramers_v:.4f}\")\n",
        "print(\"  Interpretation:\")\n",
        "if cramers_v < 0.1:\n",
        "    print(\"    Very weak association\")\n",
        "elif cramers_v < 0.3:\n",
        "    print(\"    Weak association\")\n",
        "elif cramers_v < 0.5:\n",
        "    print(\"    Moderate association\")\n",
        "else:\n",
        "    print(\"    Strong association\")\n",
        "\n",
        "# Identify clusters strongly associated with specific outcomes\n",
        "print(\"\\n=== Clusters Strongly Associated with Outcomes ===\")\n",
        "for outcome in ['Denied', 'Cancelled', 'Pending']:\n",
        "    if outcome in cluster_outcome_crosstab.columns:\n",
        "        # Find clusters where this outcome is dominant (>50% of cases)\n",
        "        dominant_clusters = cluster_outcome_crosstab[cluster_outcome_crosstab[outcome] > 0.5]\n",
        "        if len(dominant_clusters) > 0:\n",
        "            print(f\"\\n{outcome} (>50% of cases):\")\n",
        "            for cluster_id, row in dominant_clusters.iterrows():\n",
        "                proportion = row[outcome]\n",
        "                count = cluster_outcome_counts.loc[cluster_id, outcome]\n",
        "                total_in_cluster = cluster_outcome_counts.loc[cluster_id].sum()\n",
        "                print(f\"  Cluster {cluster_id}: {proportion:.1%} ({count}/{total_in_cluster} cases)\")\n",
        "\n",
        "# Save statistical test results\n",
        "stats_results = pd.DataFrame({\n",
        "    'chi2': [chi2],\n",
        "    'p_value': [p_value],\n",
        "    'degrees_of_freedom': [dof],\n",
        "    'cramers_v': [cramers_v]\n",
        "})\n",
        "stats_results.to_csv(output_dir / 'cluster_outcome_statistical_test.csv', index=False)\n",
        "print(\"\\nSaved: cluster_outcome_statistical_test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare activity patterns across outcome groups\n",
        "# Get top activities across all outcomes\n",
        "all_activities_flat = []\n",
        "for seq in case_data['activity_sequence']:\n",
        "    all_activities_flat.extend(seq)\n",
        "top_activities_all = pd.Series(all_activities_flat).value_counts().head(20).index.tolist()\n",
        "\n",
        "# Create activity frequency matrix per outcome\n",
        "activity_freq_by_outcome = []\n",
        "for outcome in ['Denied', 'Cancelled', 'Pending', 'Other']:\n",
        "    if outcome in case_data['outcome'].values:\n",
        "        outcome_cases = case_data[case_data['outcome'] == outcome]\n",
        "        all_activities_in_outcome = []\n",
        "        for seq in outcome_cases['activity_sequence']:\n",
        "            all_activities_in_outcome.extend(seq)\n",
        "        activity_counts = pd.Series(all_activities_in_outcome).value_counts()\n",
        "        total_activities = len(all_activities_in_outcome)\n",
        "        \n",
        "        frequencies = [activity_counts.get(activity, 0) / total_activities if total_activities > 0 else 0 \n",
        "                      for activity in top_activities_all]\n",
        "        activity_freq_by_outcome.append(frequencies)\n",
        "    else:\n",
        "        activity_freq_by_outcome.append([0] * len(top_activities_all))\n",
        "\n",
        "activity_freq_df_outcome = pd.DataFrame(activity_freq_by_outcome, \n",
        "                                        index=['Denied', 'Cancelled', 'Pending', 'Other'],\n",
        "                                        columns=top_activities_all)\n",
        "\n",
        "# Create heatmap\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "sns.heatmap(activity_freq_df_outcome, annot=False, fmt='.3f', cmap='YlOrRd', \n",
        "           cbar_kws={'label': 'Activity Frequency'}, ax=ax)\n",
        "ax.set_xlabel('Activity', fontsize=12)\n",
        "ax.set_ylabel('Outcome', fontsize=12)\n",
        "ax.set_title('Activity Pattern Heatmap by Outcome (Top 20 Activities)', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'activity_patterns_by_outcome.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: activity_patterns_by_outcome.png\")\n",
        "\n",
        "# Identify activities characteristic of each outcome\n",
        "print(\"\\n=== Activities Characteristic of Each Outcome ===\")\n",
        "for outcome in ['Denied', 'Cancelled', 'Pending']:\n",
        "    if outcome in activity_freq_df_outcome.index:\n",
        "        outcome_freqs = activity_freq_df_outcome.loc[outcome]\n",
        "        # Compare to overall average\n",
        "        overall_avg = activity_freq_df_outcome.mean(axis=0)\n",
        "        # Find activities that are more frequent in this outcome than average\n",
        "        characteristic = outcome_freqs - overall_avg\n",
        "        top_characteristic = characteristic.nlargest(5)\n",
        "        print(f\"\\n{outcome} (activities more frequent than average):\")\n",
        "        for activity, diff in top_characteristic.items():\n",
        "            freq_in_outcome = outcome_freqs[activity]\n",
        "            freq_avg = overall_avg[activity]\n",
        "            print(f\"  {activity}: {freq_in_outcome:.4f} (avg: {freq_avg:.4f}, diff: {diff:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Activity pattern heatmap by cluster\n",
        "# Create activity frequency matrix per cluster\n",
        "activity_freq_matrix = []\n",
        "for cluster_id in sorted(case_data['cluster'].unique()):\n",
        "    cluster_cases = case_data[case_data['cluster'] == cluster_id]\n",
        "    all_activities_in_cluster = []\n",
        "    for seq in cluster_cases['activity_sequence']:\n",
        "        all_activities_in_cluster.extend(seq)\n",
        "    activity_counts = pd.Series(all_activities_in_cluster).value_counts()\n",
        "    total_activities = len(all_activities_in_cluster)\n",
        "    \n",
        "    frequencies = [activity_counts.get(activity, 0) / total_activities if total_activities > 0 else 0 \n",
        "                  for activity in top_activities_all]\n",
        "    activity_freq_matrix.append(frequencies)\n",
        "\n",
        "activity_freq_df = pd.DataFrame(activity_freq_matrix, \n",
        "                                index=[f'Cluster {i}' for i in sorted(case_data['cluster'].unique())],\n",
        "                                columns=top_activities_all)\n",
        "\n",
        "# Annotate clusters with dominant outcome\n",
        "cluster_labels_with_outcome = []\n",
        "for cluster_id in sorted(case_data['cluster'].unique()):\n",
        "    dominant_outcome = cluster_outcome_crosstab.loc[cluster_id].idxmax()\n",
        "    prop = cluster_outcome_crosstab.loc[cluster_id, dominant_outcome]\n",
        "    cluster_labels_with_outcome.append(f'Cluster {cluster_id}\\n({dominant_outcome}: {prop:.1%})')\n",
        "\n",
        "activity_freq_df.index = cluster_labels_with_outcome\n",
        "\n",
        "# Create heatmap\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "sns.heatmap(activity_freq_df, annot=False, fmt='.3f', cmap='YlOrRd', \n",
        "           cbar_kws={'label': 'Activity Frequency'}, ax=ax)\n",
        "ax.set_xlabel('Activity', fontsize=12)\n",
        "ax.set_ylabel('Cluster (Dominant Outcome)', fontsize=12)\n",
        "ax.set_title('Activity Pattern Heatmap by Cluster (Top 20 Activities)\\nClusters annotated with dominant outcome', \n",
        "            fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_dir / 'cluster_activity_patterns_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: cluster_activity_patterns_heatmap.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
